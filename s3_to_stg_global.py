# Query: 
# ContextLines: 1

#!/usr/bin/python
# -*- coding: utf-8 -*-
# Databricks notebook 
sourcedbutils.widgets.text("META_CATALOG", "")dbutils.widgets.text("META_SCHEMA", "")dbutils.widgets.text("LOG_BUCKET_PATH","")

# COMMAND ----------# MAGIC %md# MAGIC ### Calling Custom Logger function defined in another notebook to use logging capabilities.

# MAGIC We cannot parameterize the path of the notebook so, carefully specify the path before committing.# COMMAND ----------
# MAGIC %run /Repos/jonathan.manalo@movista.com/data-vault/Databricks/Logger/CustomLogger $LOG_BUCKET_PATH=$LOG_BUCKET_PATH

# COMMAND ----------
META_CATALOG = dbutils.widgets.get("META_CATALOG")
META_SCHEMA = dbutils.widgets.get("META_SCHEMA")
ENV = META_CATALOG.split("_")[0]CLIENT = META_SCHEMA

# COMMAND ----------
# MAGIC %md# MAGIC > Notebook parameters such as ***META_DATA, ENV*** are both mandatory for this function.

# MAGIC # MAGIC ##Function defined for writing data from S3 to stage layer# MAGIC ---# MAGIC # MAGIC *This function is called when declaring stream for each table*# MAGIC # MAGIC - ###This function takes **two** parameters :# MAGIC 1. Dataframe - converting AVRO file# MAGIC 2. BATCHID/epoch_id is taken automatically for each 1M records# MAGIC # MAGIC - ###This function will insert data into target stage layer table.# COMMAND ----------# Previous_Version - S3_TO_STAGE_19.01.2023# Current Version - S3_TO_STAGE_30.03.2023# Autors - QBrainx dev teamfrom pyspark.sql.functions 

import * from delta.tables import *from pyspark.sql.window import *from datetime import timedelta

# This function writes data to target stage tables using dataframes generated from AVRO files# inputs - DataFrame, epoch_id(autogenerated)# output - Voiddef 

S3_to_Stage_TL(raw_df, epoch_id): 
    try: # Persisting dataframe to contain in a cache raw_df.persist() # Checking dataframe is not empty - when evaluates to true the process will proceed 
    if raw_df.count() > 0: tbl = raw_df.select("source.table").limit(1).first()["table"] FILE_PATH = raw_df.select("filePath").distinct() FILE_PATHS = [str(row.filePath) for row in FILE_PATH.collect()] filePaths = ",".join(FILE_PATHS) log_info( "S3_TO_STG_AutoLoader", "Process Started S3 to STAGE ETL for table :{}".format(tbl), "S3_to_Stage_TL", [{"target_stage_table": tbl, "environment": ENV, "client": CLIENT}], ) 
    # AVRO file has two parts before & after - we have to consider both for our process 
    # Before has data only if the record is deleted # After has data when insert is occured # Both before and after has data when the record is updated 
    # "before_select_exp" - to select data from before part of AVRO FILE 
    before_select_exp = spark.sql( "select BEFORE_SELECT_EXPRESSION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(tbl) ).collect()[0][0] BEFORE_SELECT_EXPRESSION_list = list(before_select_exp.split(",")) 
    # "after_select_exp" - to select data from after part of AVRO FILE 
    after_select_exp = spark.sql( "select AFTER_SELECT_EXPRESSION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(tbl) ).collect()[0][0] AFTER_SELECT_EXPRESSION_list = list(after_select_exp.split(","))
    
    # Here seperating the dataframe based on CDC action 
    # d - indicates the record is deleted # c or r - indicates record is inserted or created 
    # u - indicates the record is updated # "delete_df" - filters & captures deleted records got from AVRO 
    dataframe delete_df = raw_df.filter(raw_df.op == "d").selectExpr( BEFORE_SELECT_EXPRESSION_list ) 
    # "cru_df" - filters & captures inserted and updated records got from AVRO dataframe 
    cru_df = raw_df.filter( (raw_df.op == "c") | (raw_df.op == "r") | (raw_df.op == "u") ).selectExpr(AFTER_SELECT_EXPRESSION_list) 
    
    # Combining the two dataframes for easy handling df2 = cru_df.unionAll(delete_df) # getting the list of avro columns avro_col=df2.columns # Fetching the source columns that has Time datatype df3 = df2 time_datatype_col = spark.sql( "select TIME_DT_COL from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(tbl) ).collect()[0][0] # If any columns have Time datatype, then microsconds given in AVRO file is converted to hh:mm:ss format if time_datatype_col != None: time_dt_col = list(time_datatype_col.split(",")) for dtype in time_dt_col: w = Window().orderBy(dtype) mod_col_list = ( df3.select(dtype).orderBy(dtype) .rdd.map( lambda x: str(timedelta(microseconds=x[0])) if (x[0] != None) else None ) .collect() ) df3 = ( df3.withColumn( "{}_mod".format(dtype), array(*[lit(x) for x in mod_col_list]), ) .withColumn("rownum", row_number().over(w)) .withColumn( "{}_mod".format(dtype), expr("""element_at({}_mod,rownum)""".format(dtype)), ) .drop("rownum") .drop(dtype) .withColumnRenamed("{}_mod".format(dtype), dtype) ) # Fetching trget table location and audit log table location from meta data target_location = "" audit_log_location = "" target_location = spark.sql( "select concat(TARGET_TABLE_CATALOG,'.',TARGET_TABLE_SCHEMA,'.',LOWER(TABLE_NAME)) from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(tbl) ).collect()[0][0] audit_log_location = spark.sql( "select AUDIT_INFORMATION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(tbl) ).collect()[0][0] # Fetching target table datatypes and removing the CDC columns (cdc_action, cdc_timestamp, LDTS, Filepath) stage_table_schema = spark.sql( """select * from """ + target_location + """ limit 1""" ).dtypes stage_table_schema.pop(-1) stage_table_schema.pop(-1) stage_table_schema.pop(-1) stage_table_schema.pop(-1) size_of_col = len(stage_table_schema) loading_column_list = [] # Getting the list of stage table columns for cross-checking it stage_tbl_col = spark.sql( """select * from """ + target_location + """ limit 1""" ).columns # Getting AVRO list of columns for cross-checking it avro_list_of_columns = [x.upper() for x in avro_col] # Removing the last 4 meta columns list_stage_tbl_col=stage_tbl_col[:-4] list_avro_col=avro_list_of_columns[:-4] # Cross checking the list of columns col_not_in_avro=list(set(list_stage_tbl_col)-set(list_avro_col)) col_not_in_stg=list(set(list_avro_col)-set(list_stage_tbl_col)) # Finding the Timestamp & Date datatype columns from target stage table data types if len(col_not_in_avro) != 0 or len(col_not_in_stg) !=0 : raise Exception( "There are discrepencies between stage layer and AVRO structure. The list of columns not in AVRO "+str(col_not_in_avro)+ " and The list of columns not in stage "+str(col_not_in_stg) ) # The first two for loops convert unix if the columns are last of date or timestamp data types and escapes for i in range(size_of_col): if stage_table_schema[i][1] == "timestamp": loading_column_list.append( "cast(from_unixtime(" + stage_table_schema[i][0] + "/1000, 'yyyy-MM-dd HH:mm:ss.SSS') as timestamp) as " + stage_table_schema[i][0] ) # Converting unix date from AVRO to date elif stage_table_schema[i][1] == "date": loading_column_list.append( "date_from_unix_date(" + stage_table_schema[i][0] + ") as " + stage_table_schema[i][0] ) else: loading_column_list.append(stage_table_schema[i][0]) # Adding the CDC columns to dataframe loading_column_list.append("cast(op as string) as CDC_ACTION") loading_column_list.append( "cast(from_unixtime(ts_ms/1000, 'yyyy-MM-dd HH:mm:ss.SSS') as timestamp) AS CDC_TS" ) loading_column_list.append("current_timestamp() as LDTS") loading_column_list.append("filePath as FILEPATH") df_final = df3.selectExpr(loading_column_list) log_info( "S3_TO_STG_AutoLoader", "Starting to write data into table :{}".format(tbl), "S3_to_Stage_TL", [{"target_stage_table": tbl, "environment": ENV, "client": CLIENT}], ) # Writing the data to target table df_final.write.format("delta").option("mergeSchema", "true").mode( "append" ).insertInto(target_location) log_info( "S3_TO_STG_AutoLoader", "Successfully written data into table :{}".format(tbl), "S3_to_Stage_TL", [ { "target_stage_table": tbl, "rec_count": df_final.count(), "environment": ENV, "client": CLIENT, } ], ) # Getting number of data inserted and respective files df_audit_total = df_final.selectExpr("filePath").groupBy("filePath").count() tbl_s3_file = df_audit_total.select("filePath") final_s3_filelist = [str(row.filePath) for row in tbl_s3_file.collect()] stg_count_df = spark.sql( """select filePath,count(*) as cnt from """ + target_location + """ group by filePath """ ) stg_count = stg_count_df.filter( stg_count_df.filePath.isin(final_s3_filelist) ) # Capturing the source and target data count for each file # When counts match it will be "TRUE" otherwise "FALSE" in "isMatch" column audit_validation = ( df_audit_total.alias("total") .join(stg_count.alias("source"), ["filePath", "filePath"], how="inner") .selectExpr( [ "total.filePath as filePath", "total.count as COUNT_BEFORE_LOADING", "source.cnt as COUNT_AFTER_LOADING", "(case when total.count = source.cnt then True else False end) as isMatch", ] ) ) df_audit_final = ( df_audit_total.alias("mt") .join( audit_validation.alias("av"), ["filePath", "filePath"], how="inner" ) .selectExpr( [ "mt.filePath as FILENAME", "mt.count as COUNT_BEFORE_LOADING", "av.COUNT_AFTER_LOADING as COUNT_AFTER_LOADING", ] ) .withColumn("TABLE_NAME", lit(tbl)) .withColumn("LDTS", current_timestamp().cast("timestamp")) ) # "Following dataframe is to insert the audit info to audit log table" df_audit_load = df_audit_final.selectExpr( [ "TABLE_NAME", "FILENAME", "COUNT_BEFORE_LOADING", "COUNT_AFTER_LOADING", "LDTS", ] ) log_info( "S3_TO_STG_AutoLoader", "Writing data into audit log table :{}".format(audit_log_location), "S3_to_Stage_TL", [ { "audit_log_table": audit_log_location, "target_stage_table": tbl, "environment": ENV, "client": CLIENT, } ], ) # Inserting into audit log table df_audit_load.write.format("delta").option("mergeSchema", "true").mode( "append" ).partitionBy("TABLE_NAME").save(audit_log_location) isMatch_count = ( audit_validation.select() .filter(audit_validation.isMatch == False) .count() ) # Checking any abnormal inserts (i.e source data count and target data count difference) # If "FALSE" is captured in "isMatch" column of audit log -> error logs are inserted and exception will be raised if isMatch_count > 0: Error_tbl = tbl df_file_path = audit_validation.select("filePath").filter( audit_validation.isMatch == False ) FILE_PATHS = [str(row.filePath) for row in df_file_path.collect()] error_log_location = spark.sql( "select ERROR_INFORMATION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(Error_tbl) ).collect()[0][0] log_error( "S3_TO_STG_AutoLoader", "Error occured while loading into table :{}".format(tbl), "S3_to_Stage_TL", [ { "target_stage_table": tbl, "source_avro_filePaths": ",".join(FILE_PATHS), "error_statement": "Total inserted records does not match the total records from the source", "environment": ENV, "client": CLIENT, } ], ) # Inserting into error log table for FILE_PATH in FILE_PATHS: spark.sql( """insert into delta.`""" + error_log_location + """` values('""" + Error_tbl + """','""" + str(FILE_PATH) + """','""" + "Total inserted records does not match the total records from the source." + """',current_timestamp()) """ ) # Collecting count mismatched filepaths error_file_to_remove = audit_validation.select("filePath").filter( audit_validation.isMatch == False ) final_error_file_to_remove = [ str(row.filePath) for row in error_file_to_remove.collect() ] log_warning( "S3_TO_STG_AutoLoader", "Removing bad data from stage table :{}".format(tbl), "S3_to_Stage_TL", [ { "target_stage_table": tbl, "source_avro_filePaths": ",".join(FILE_PATHS), "bad_files": ",".join(final_error_file_to_remove), "environment": ENV, "client": CLIENT, } ], ) # Deleting data of those count mismatched filepaths for final_error_files in final_error_file_to_remove: Delta_tbl = spark.sql( """delete from """ + target_location + """ where filepath =\'""" + final_error_files + """\' """ ) raise ValueError( "Total inserted records doesn't match the total records from the source." ) # After inserting into stage layer unpersisting the dataframe from cache raw_df.unpersist() # The following block will raise exception if anything goes bad on initial stage # Abnormalities like, source & target structure mismatch, wrong source file etc., will fail process except ValueError as ve: log_warning( "S3_TO_STG_AutoLoader", "Raising an exception to prevent stream to further processing : {}".format( tbl ), "S3_to_Stage_TL", [{"target_stage_table": tbl, "environment": ENV, "client": CLIENT}], ) raise Exception( "Total inserted records doesn't match the total records from the source." ) except Exception as e: FILE_PATH = raw_df.select("filePath") ERROR_FILE_PATHS = [str(row.filePath) for row in FILE_PATH.collect()] filePaths = ",".join(ERROR_FILE_PATHS) log_error( "S3_TO_STG_AutoLoader", "Error occured while loading into table :{}".format(tbl), "S3_to_Stage_TL", [ { "target_stage_table": tbl, "source_avro_filePaths": filePaths, "error_statement": str(e).replace("'", ""), "environment": ENV, "client": CLIENT, } ], ) error_log_location = spark.sql( "select ERROR_INFORMATION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(tbl) ).collect()[0][0] log_info( "S3_TO_STG_AutoLoader", "Writing data into error log table :{}".format(audit_log_location), "S3_to_Stage_TL", [ { "target_stage_table": tbl, "error_log_table": error_log_location, "error_avro_filePaths": ",".join(ERROR_FILE_PATHS), "environment": ENV, "client": CLIENT, } ], ) # Captured error is/are inserted to audit log table for ERROR_FILE_PATH in ERROR_FILE_PATHS: spark.sql( """insert into delta.`""" + error_log_location + """` values('""" + tbl + """','""" + str(ERROR_FILE_PATH) + """','""" + str(e).replace("'", "") + """',current_timestamp()) """ ) log_warning( "S3_TO_STG_AutoLoader", "Raising an exception to prevent stream to further processing : {}".format( tbl ), "S3_to_Stage_TL", [ { "target_stage_table": tbl, "error_avro_filePaths": ",".join(ERROR_FILE_PATHS), "environment": ENV, "client": CLIENT, } ], ) raise Exception( "Error occured while loading into " + tbl + ". Please check ERROR_LOG Table." )# COMMAND ----------# MAGIC %md# MAGIC > Notebook parameters such as ***META_DATA,ENV*** are both mandatory for this function.# MAGIC ## Function Defined for Spark Structured Streaming workloads.# MAGIC ---# MAGIC # MAGIC *This function takes table_name as input and creates a the stream for the respective table*# MAGIC # MAGIC - ###This function takes **one** parameter :# MAGIC 1. TABLE_NAME - which is the source table name. # MAGIC # MAGIC - ###This function will create streams which helps to capture real-time changes in source table and writes into target Stage tables.# COMMAND ----------
    
    # This function will initialize spark streaming for each table# Inputs - Table name
    # Output - Voiddef stage_streaming(table_name): 
    # Following select expressions will fetch meta data associated for given "table_name" 
    TABLE_NAME = table_name PARTITIONED_COLUMNS = spark.sql( "select PARTITIONED_COLUMNS from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] SCHEMA_LOCATION = spark.sql( "select SCHEMA_LOCATION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] SOURCE_BUCKET_PATH = spark.sql( "select SOURCE_BUCKET_PATH from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] CHECKPOINT_LOCATION = spark.sql( "select CHECKPOINT_LOCATION from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] FILE_NOTIFICATION_MODE = spark.sql( "select FILE_NOTIFICATION_MODE from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] BACKFILL_INTERVAL = spark.sql( "select BACKFILL_INTERVAL from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] SCHEMA_EVOLUTION_MODE = spark.sql( "select SCHEMA_EVOLUTION_MODE from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] INFER_COLUMN_TYPES = spark.sql( "select INFER_COLUMN_TYPES from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] FILE_FORMAT = spark.sql( "select FILE_FORMAT from " + META_CATALOG + "." + META_SCHEMA + ".META_TABLE_S3_DETAILS where TABLE_NAME='{}'".format(TABLE_NAME) ).collect()[0][0] log_info( "S3_TO_STG_AutoLoader", "Starting Autoloader Streaming process for table :{}".format(table_name), "stage_streaming", [ { "source_bucket_path": SOURCE_BUCKET_PATH, "target_stage_table": table_name, "environment": ENV, "client": CLIENT, } ], ) 
    
    # This block will start streaming using the meta data fetched for given table 
    ( spark.readStream.format("cloudFiles") 
    .option("cloudFiles.validateOptions", "true") 
    .option("cloudFiles.format", FILE_FORMAT) 
    .option("cloudFiles.inferColumnTypes", INFER_COLUMN_TYPES) 
    .option("cloudFiles.partitionColumns", PARTITIONED_COLUMNS) 
    .option("cloudFiles.schemaLocation", SCHEMA_LOCATION) 
    .option("cloudFiles.useNotifications", FILE_NOTIFICATION_MODE) 
    .option("cloudFiles.backfillInterval", BACKFILL_INTERVAL) 
    .option("cloudFiles.schemaEvolutionMode", SCHEMA_EVOLUTION_MODE) 
    .load(SOURCE_BUCKET_PATH) 
    .withColumn("filePath", input_file_name()) 
    .writeStream.option("checkpointLocation", CHECKPOINT_LOCATION) 
    .queryName(TABLE_NAME + "_S3_to_STG") 
    .foreachBatch(S3_to_Stage_TL) 
    .start() )
